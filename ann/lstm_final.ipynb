{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_data = pd.read_csv(\"../gym-unbalanced-disk/disc-benchmark-files/training-val-test-data.csv\", header=None)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "ulist = training_data.iloc[:, 0].tolist()[1:]   # Input voltage\n",
    "ylist = training_data.iloc[:, 1].tolist()[1:]  # Output angle\n",
    "\n",
    "# Convert elements to floats\n",
    "ulist = [float(val) for val in ulist]\n",
    "ylist = [float(val) for val in ylist]\n",
    "\n",
    "# Normalize the data\n",
    "scaler_u = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "ulist = np.array(ulist).reshape(-1, 1)\n",
    "ylist = np.array(ylist).reshape(-1, 1)\n",
    "\n",
    "ulist_scaled = scaler_u.fit_transform(ulist)\n",
    "ylist_scaled = scaler_y.fit_transform(ylist)\n",
    "\n",
    "def make_OE_data(udata, ydata, nf=100):\n",
    "    U = [] \n",
    "    Y = [] \n",
    "    for k in range(nf, len(udata) + 1):\n",
    "        U.append(udata[k - nf:k]) #a)\n",
    "        Y.append(ydata[k - nf:k]) #a)\n",
    "    return np.array(U), np.array(Y)\n",
    "\n",
    "nfuture = 50\n",
    "U_LSTM, Y_LSTM = make_OE_data(ulist_scaled.flatten(), ylist_scaled.flatten(), nf=nfuture)\n",
    "\n",
    "U_LSTM = torch.tensor(U_LSTM, dtype=torch.float64).unsqueeze(-1).clone().detach()  # shape (batch_size, sequence_length, input_size)\n",
    "Y_LSTM = torch.tensor(Y_LSTM, dtype=torch.float64).clone().detach()\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(U_LSTM))\n",
    "\n",
    "Utrain, Ytrain_LSTM = U_LSTM[:split_index], Y_LSTM[:split_index]\n",
    "Uval, Yval_LSTM = U_LSTM[split_index:], Y_LSTM[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Layers LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_2 = int(2*hidden_size)\n",
    "\n",
    "        self.input_size = 1\n",
    "        self.output_size = 1\n",
    "        \n",
    "        net = lambda n_in, n_out: nn.Sequential( \n",
    "            nn.Linear(n_in, 128),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(128, 64),  \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, n_out)   \n",
    "        )\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size_2, batch_first=True).double()\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_size_2, hidden_size=self.hidden_size_2, batch_first=True).double()\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_size_2, hidden_size=hidden_size, batch_first=True).double()\n",
    "        \n",
    "        self.h2o = net(hidden_size + self.input_size, self.output_size).double() \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hiddens1, (h_n1, c_n1) = self.lstm1(inputs)\n",
    "        hiddens2, (h_n2, c_n2) = self.lstm2(hiddens1)\n",
    "        hiddens3, (h_n3, c_n3) = self.lstm3(hiddens2)\n",
    "        \n",
    "        combined = torch.cat((hiddens3, inputs), dim=2)\n",
    "        h2o_input = combined.view(-1, self.hidden_size + self.input_size)\n",
    "        y_predict = self.h2o(h2o_input).view(inputs.shape[0], inputs.shape[1])\n",
    "        \n",
    "        return y_predict\n",
    "\n",
    "# 4 Layers LSTM\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.hidden_size_2 = int(2*hidden_size)\n",
    "#         self.hidden_size_3 = int(4*hidden_size)\n",
    "\n",
    "#         self.input_size = 1\n",
    "#         self.output_size = 1\n",
    "        \n",
    "#         net = lambda n_in, n_out: nn.Sequential( \n",
    "#             nn.Linear(n_in, 128),  # Add a hidden layer\n",
    "#             nn.ReLU(),  # Activation function\n",
    "#             nn.Linear(128, 64),  # Another hidden layer\n",
    "#             nn.ReLU(),  # Activation function\n",
    "#             nn.Linear(64, n_out)   # Output layer\n",
    "#         )\n",
    "        \n",
    "#         self.lstm1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size_2, batch_first=True).double()\n",
    "#         self.lstm2 = nn.LSTM(input_size=self.hidden_size_2, hidden_size=self.hidden_size_3, batch_first=True).double()\n",
    "#         self.lstm3 = nn.LSTM(input_size=self.hidden_size_3, hidden_size=self.hidden_size_2, batch_first=True).double()\n",
    "#         self.lstm4 = nn.LSTM(input_size=self.hidden_size_2, hidden_size=hidden_size, batch_first=True).double()\n",
    "        \n",
    "#         self.h2o = net(hidden_size + self.input_size, self.output_size).double() \n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         hiddens1, (h_n1, c_n1) = self.lstm1(inputs)\n",
    "#         hiddens2, (h_n2, c_n2) = self.lstm2(hiddens1)\n",
    "#         hiddens3, (h_n3, c_n3) = self.lstm3(hiddens2)\n",
    "#         hiddens4, (h_n3, c_n3) = self.lstm4(hiddens3)\n",
    "        \n",
    "        \n",
    "#         combined = torch.cat((hiddens4, inputs), dim=2)\n",
    "#         h2o_input = combined.view(-1, self.hidden_size + self.input_size)\n",
    "#         y_predict = self.h2o(h2o_input).view(inputs.shape[0], inputs.shape[1])\n",
    "        \n",
    "#         return y_predict\n",
    "\n",
    "n_hidden_nodes = 64\n",
    "n_burn = 10\n",
    "epochs = 100\n",
    "batch_size = 32  \n",
    "lr = 0.001\n",
    "\n",
    "model_name= f\"LSTM_2layers_{nfuture}nfuture_{n_burn}n_burn_{lr}lr.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(n_hidden_nodes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.1, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(Utrain), batch_size):\n",
    "        Uin = Utrain[i:i + batch_size]\n",
    "        Y_real = Ytrain_LSTM[i:i + batch_size]\n",
    "        \n",
    "        Y_predict = model(Uin)\n",
    "\n",
    "        residual = Y_real - Y_predict\n",
    "        Loss = torch.mean(residual[:, n_burn:]**2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Loss_val = torch.mean((model(Uval)[:, n_burn:] - Yval_LSTM[:, n_burn:])**2)**0.5\n",
    "        Loss_train = torch.mean((model(Utrain)[:, n_burn:] - Ytrain_LSTM[:, n_burn:])**2)**0.5\n",
    "        print(f'epoch={epoch}, Validation NRMS={Loss_val.item():.2%}, Train NRMS={Loss_train.item():.2%}')\n",
    "        scheduler.step(Loss_val)\n",
    "        val_losses.append(Loss_val.item())\n",
    "        train_losses.append(Loss_train.item())\n",
    "\n",
    "        if Loss_val < best_val_loss:\n",
    "            best_val_loss = Loss_val\n",
    "            #torch.save(model.state_dict(), model_name)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), model_name)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the losses\n",
    "plt.plot(range(epochs), [loss for loss in train_losses], label='Training Loss')\n",
    "plt.plot(range(epochs), [loss for loss in val_losses], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (%)')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train prediction errors:\n",
      "RMS: 0.07485682194341 radians\n",
      "RMS: 4.288979965119682 degrees\n",
      "NRMS: 63.69137266580939 %\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (645x29 and 1x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m Xtest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Xtest, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Make predictions on test data\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m Ypredict \u001b[38;5;241m=\u001b[39m \u001b[43mreg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m Ypredict \u001b[38;5;241m=\u001b[39m Ypredict\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Save predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vddal\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vddal\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 46\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 46\u001b[0m     hiddens1, (h_n1, c_n1) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     hiddens2, (h_n2, c_n2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(hiddens1)\n\u001b[0;32m     48\u001b[0m     hiddens3, (h_n3, c_n3) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm3(hiddens2)\n",
      "File \u001b[1;32mc:\\Users\\vddal\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vddal\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vddal\\anaconda3\\envs\\experiments\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (645x29 and 1x512)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Load training data\n",
    "out = np.load('../gym-unbalanced-disk/disc-benchmark-files/training-val-test-data.npz')\n",
    "u_train = out['u']  # Input voltage\n",
    "th_train = out['th']  # Output angle\n",
    "\n",
    "scaler_u = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "u_train = np.array(u_train).reshape(-1, 1)\n",
    "th_train = np.array(th_train).reshape(-1, 1)\n",
    "\n",
    "u_train = scaler_u.fit_transform(u_train)\n",
    "th_train = scaler_y.fit_transform(th_train)\n",
    "\n",
    "\n",
    "nfuture = 50 \n",
    "U_LSTM, Y_LSTM = make_OE_data(u_train, th_train, nf=nfuture)\n",
    "\n",
    "\n",
    "U_LSTM = torch.tensor(U_LSTM, dtype=torch.float64)\n",
    "Y_LSTM = torch.tensor(Y_LSTM, dtype=torch.float64)\n",
    "\n",
    "reg = LSTM(n_hidden_nodes)\n",
    "reg.load_state_dict(torch.load(model_name))\n",
    "reg.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    Ytrain_pred = reg(U_LSTM)\n",
    "\n",
    "Ytrain_pred = Ytrain_pred.detach().numpy().flatten()\n",
    "Y_LSTM = Y_LSTM.numpy().flatten()\n",
    "\n",
    "print('train prediction errors:')\n",
    "print('RMS:', np.mean((Ytrain_pred - Y_LSTM)**2)**0.5, 'radians')\n",
    "print('RMS:', np.mean((Ytrain_pred - Y_LSTM)**2)**0.5 / (2 * np.pi) * 360, 'degrees')\n",
    "print('NRMS:', np.mean((Ytrain_pred - Y_LSTM)**2)**0.5 / Y_LSTM.std() * 100, '%')\n",
    "\n",
    "\n",
    "data = np.load('../gym-unbalanced-disk/disc-benchmark-files/hidden-test-prediction-submission-file.npz')\n",
    "upast_test = data['upast']  # N by u[k-15],u[k-14],...,u[k-1]\n",
    "thpast_test = data['thpast']  # N by y[k-15],y[k-14],...,y[k-1]\n",
    "\n",
    "Xtest = np.concatenate([upast_test[:, 1:], thpast_test], axis=1)\n",
    "Xtest = torch.tensor(Xtest, dtype=torch.float64)\n",
    "\n",
    "Ypredict = reg(Xtest)\n",
    "Ypredict = Ypredict.detach().numpy().flatten()\n",
    "\n",
    "# Save predictions\n",
    "np.savez('../gym-unbalanced-disk/disc-benchmark-files/hidden-test-prediction-example-submission-file.npz', upast=upast_test, thpast=thpast_test, thnow=Ypredict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train simulation errors:\n",
      "RMS: 0.5817430545176709 radians\n",
      "RMS: 33.331421784911505 degrees\n",
      "NRMS: 121.36402278413567 %\n"
     ]
    }
   ],
   "source": [
    "# Simulation\n",
    "out = np.load('../gym-unbalanced-disk/disc-benchmark-files/training-val-test-data.npz')\n",
    "th_train = out['th'] #th[0],th[1],th[2],th[3],...\n",
    "u_train = out['u'] #u[0],u[1],u[2],u[3],...\n",
    "\n",
    "data = np.load('../gym-unbalanced-disk/disc-benchmark-files/hidden-test-simulation-submission-file.npz')\n",
    "u_test = data['u']\n",
    "th_test = data['th'] #only the first 50 values are filled the rest are zeros\n",
    "\n",
    "\n",
    "nfuture = 50\n",
    "convert = lambda x: [torch.tensor(xi,dtype=torch.float64) for xi in x]\n",
    "U_LSTM, Y_LSTM = convert(make_OE_data(u_train, th_train, nf=nfuture))\n",
    "\n",
    "reg = LSTM(n_hidden_nodes) \n",
    "reg.load_state_dict(torch.load(model_name))\n",
    "reg.eval()\n",
    "\n",
    "# Simulation\n",
    "out = np.load('../gym-unbalanced-disk/disc-benchmark-files/training-val-test-data.npz')\n",
    "th_train = out['th']\n",
    "u_train = out['u']\n",
    "\n",
    "data = np.load('../gym-unbalanced-disk/disc-benchmark-files/hidden-test-simulation-submission-file.npz')\n",
    "u_test = data['u']\n",
    "th_test = data['th']\n",
    "nfuture = 50\n",
    "convert = lambda x: [torch.tensor(xi, dtype=torch.float64) for xi in x]\n",
    "U_LSTM, Y_LSTM = convert(make_OE_data(u_train, th_train, nf=nfuture))\n",
    "\n",
    "def fmodel_OE(upast):\n",
    "    x = torch.as_tensor(upast, dtype=torch.float64).unsqueeze(0).unsqueeze(-1)  \n",
    "    with torch.no_grad():\n",
    "        ypred = reg(x)  \n",
    "    return ypred.numpy().flatten()[-1]  \n",
    "\n",
    "# OE simulation function\n",
    "def OE_Simulation(U_data, f, nf, skip):\n",
    "    U_past = U_data[:skip]  \n",
    "    U_past = U_past.tolist() \n",
    "    Y_sim = U_data[:skip].tolist() \n",
    "\n",
    "    for i in range(skip, len(U_data)):\n",
    "        y_current = f(U_past[-nf:])  \n",
    "        Y_sim.append(y_current)  \n",
    "        U_past.append(U_data[i])\n",
    "\n",
    "    return np.array(Y_sim)\n",
    "\n",
    "skip = 50\n",
    "\n",
    "# OE simulation on training data\n",
    "th_train_sim = OE_Simulation(u_train, fmodel_OE, 50, skip=skip)\n",
    "\n",
    "print('train simulation errors:')\n",
    "print('RMS:', np.mean((th_train_sim[skip:] - th_train[skip:])**2)**0.5, 'radians')\n",
    "print('RMS:', np.mean((th_train_sim[skip:] - th_train[skip:])**2)**0.5 / (2 * np.pi) * 360, 'degrees')\n",
    "print('NRMS:', np.mean((th_train_sim[skip:] - th_train[skip:])**2)**0.5 / th_train.std() * 100, '%')\n",
    "\n",
    "# OE simulation on test data\n",
    "th_test_sim = OE_Simulation(u_test, fmodel_OE, 50, skip=skip)\n",
    "\n",
    "assert len(th_test_sim) == len(th_test)\n",
    "np.savez('../gym-unbalanced-disk/disc-benchmark-files/hidden-test-simulation-example-submission-file.npz', th=th_test_sim, u=u_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise Set 3 Deep Learning for S&C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
